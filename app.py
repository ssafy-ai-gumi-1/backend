import os
import json
import random

from openai import OpenAI
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query
from langchain.chains import RetrievalQA
from langchain_upstage import UpstageEmbeddings
from langchain_upstage import ChatUpstage
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
from pydantic import BaseModel

load_dotenv()

chat_upstage = ChatUpstage()
embedding_upstage = UpstageEmbeddings(model="embedding-query")

pinecone_api_key = os.environ.get("PINECONE_API_KEY")
openai_api_key = os.environ.get("OPENAI_API_KEY")
openai_client = OpenAI(api_key=openai_api_key)

pc = Pinecone(api_key=pinecone_api_key)
index_name = "cs-interview-index"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=4096,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )

pinecone_vectorstore = PineconeVectorStore(index=pc.Index(index_name), embedding=embedding_upstage)

pinecone_retriever = pinecone_vectorstore.as_retriever(
    search_type='mmr',
    search_kwargs={"k": 3}
)

app = FastAPI()

from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


with open("qa_data.json", "r", encoding="utf-8") as f:
    qa_data = json.load(f)

@app.get("/random-question")
async def random_question(category: str = Query(default=None)):
    filtered = [q for q in qa_data if category is None or q["category"] == category]

    if not filtered:
        return {"error": "í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."}

    sample = random.choice(filtered)

    return {
        "question": sample["question"],
        "category": sample["category"],
        "keywords": sample["keywords"]
    }


class EvaluateRequest(BaseModel):
    question: str
    answer: str

@app.post("/evaluate")
async def evaluate(req: EvaluateRequest):
    question = req.question.strip()
    user_answer = req.answer.strip()

    matched = next((q for q in qa_data if q["question"].strip() == question), None)
    if not matched:
        raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ ê¸°ì¤€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    ground_truth = matched["answer"]

    retrieved_docs = pinecone_retriever.get_relevant_documents(question)
    contexts = [doc.page_content for doc in retrieved_docs]
    
    if ground_truth not in contexts:
        contexts.append(ground_truth) 
  
    prompt = build_feedback_prompt(
        question=question,
        user_answer=user_answer,
        context_list=contexts,
        ground_truth=ground_truth
    )

    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": build_system_prompt()},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )

    content = response.choices[0].message.content
    if content is None:
        raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    feedback = content.strip()
    
    return {"reply": feedback}

def build_system_prompt():
    return (
        "ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ê°œë°œìì´ë©°, í›„ë°° ê°œë°œìì˜ ê¸°ìˆ  ë©´ì ‘ì„ ë„ì™€ì£¼ëŠ” ë©´ì ‘ ì½”ì¹˜ì…ë‹ˆë‹¤. "
        "ì¹œì ˆí•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ í”¼ë“œë°±ì„ ì£¼ê³ , ë„ˆë¬´ ê³µê²©ì ì´ì§€ ì•Šê²Œ ê°œì„ ì ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    )

def build_feedback_prompt(question, user_answer, context_list, ground_truth):
    context_text = "\n".join(context_list)

    prompt = f"""
ë‹¹ì‹ ì€ ê°œë°œì ë©´ì ‘ ì½”ì¹˜ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì§ˆë¬¸ê³¼ ì‚¬ìš©ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ì—¬, ì‹¤ì „ ë©´ì ‘ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ë…¼ë¦¬ì ì¸ í”¼ë“œë°±ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.  
ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ì„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±í•´ ì£¼ì„¸ìš”:

1. ì‚¬ìš©ìê°€ ì–´ë–¤ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì˜ ì–¸ê¸‰í–ˆëŠ”ì§€  
2. ì–´ë–¤ í•µì‹¬ ê°œë…ì´ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ (ë¬¸ë§¥ ë˜ëŠ” ëª¨ë²”ë‹µì•ˆ ê¸°ì¤€)  
3. ë©´ì ‘ ìŠ¤íƒ€ì¼(ì„¤ëª… ìˆœì„œ, í‘œí˜„ ë°©ì‹ ë“±)ì—ì„œ ì–´ë–»ê²Œ ë³´ì™„í•˜ë©´ ì¢‹ì„ì§€  

---

  **ì¤‘ìš” ì§€ì¹¨** (ì ˆëŒ€ ìœ„ë°˜í•˜ì§€ ë§ˆì„¸ìš”):

- ì‚¬ìš©ìì˜ ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë‹¨ì–´ë§Œ ë‚˜ì—´ë˜ì–´ ìˆìœ¼ë©´, **ì´í•´ë„ë¥¼ íŒë‹¨í•˜ê¸° ì–´ë µë‹¤ê³  ëª…í™•í•˜ê²Œ ì§€ì **í•´ì•¼ í•©ë‹ˆë‹¤.  
  â†’ "í•µì‹¬ ìš©ì–´ëŠ” ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ ì„¤ëª…ì´ ì—†ì–´ ë©´ì ‘ì—ì„œ ë‚®ì€ í‰ê°€ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤." ë¼ê³  í‰ê°€í•´ ì£¼ì„¸ìš”.

- ì‚¬ìš©ìê°€ ë§í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ **ì¶”ë¡ í•˜ê±°ë‚˜ ê³¼í•˜ê²Œ ê¸ì •ì ìœ¼ë¡œ ë§í•˜ì§€ ë§ˆì„¸ìš”.**  
  â†’ ë°˜ë“œì‹œ ì‹¤ì œ ë‹µë³€ ë‚´ìš©ì— **í•œì •í•´ì„œ** í‰ê°€í•´ ì£¼ì„¸ìš”.

- í”¼ë“œë°±ì€ ì§€ì ê³¼ ê²©ë ¤ì˜ ê· í˜•ì„ ë§ì¶°ì„œ, ì„±ì¥í•  ìˆ˜ ìˆë„ë¡ **êµ¬ì²´ì ì´ê³  ë”°ëœ»í•˜ê²Œ** ì‘ì„±í•´ì£¼ì„¸ìš”.

---

ğŸ“Œ ì§ˆë¬¸:  
{question}

âœï¸ ì‚¬ìš©ì ë‹µë³€:  
{user_answer}

ë°˜ë“œì‹œ ì‚¬ìš©ì ë‹µë³€ìœ¼ë¡œë§Œ íŒë‹¨í•´ì„œ ë‹µë³€í•˜ì„¸ìš”. ë°˜ë“œì‹œ!!!!!

ğŸ“š ê²€ìƒ‰ëœ ë¬¸ë§¥ ì •ë³´:  
{context_text}

âœ… ëª¨ë²”ë‹µë³€:  
{ground_truth}
"""
    return prompt



@app.get("/health")
@app.get("/")
async def health_check():
    return {"status": "ok"}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)

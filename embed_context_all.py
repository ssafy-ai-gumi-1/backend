import os
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredMarkdownLoader
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)

# 2. ì„ë² ë”© ëª¨ë¸
embedding_upstage = UpstageEmbeddings(model="embedding-query")

# 3. í…ìŠ¤íŠ¸ ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# 4. ë©”íƒ€ë°ì´í„° íŒŒì‹± í•¨ìˆ˜
def extract_metadata_from_md(path: str):
    with open(path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    category = None
    keywords = []

    for line in lines:
        line = line.strip()
        if line.startswith("ì¹´í…Œê³ ë¦¬:"):
            category = line.replace("ì¹´í…Œê³ ë¦¬:", "").strip()
        elif line.startswith("í‚¤ì›Œë“œ:"):
            keyword_raw = line.replace("í‚¤ì›Œë“œ:", "").strip()
            keyword_raw = keyword_raw.strip("[]")
            keywords = [kw.strip() for kw in keyword_raw.split(",") if kw.strip()]
        if category and keywords:
            break

    return category, keywords

# 5. ì „ì²´ md íŒŒì¼ì„ í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ë¡œ ì—…ë¡œë“œ
def upload_all_to_single_index(base_dir: str, index_name: str):
    all_docs = []

    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".md"):
                path = os.path.join(root, file)
                print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {path}")

                category, keywords = extract_metadata_from_md(path)
                loader = UnstructuredMarkdownLoader(path)
                docs = loader.load()
                split_docs = text_splitter.split_documents(docs)

                for doc in split_docs:
                    doc.metadata["category"] = category
                    doc.metadata["keyword"] = keywords

                all_docs.extend(split_docs)

    # ì¸ë±ìŠ¤ ìƒì„±
    if index_name not in pc.list_indexes().names():
        pc.create_index(
            name=index_name,
            dimension=4096,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )

    print(f"ğŸ“¦ ì´ {len(all_docs)} chunks ì—…ë¡œë“œ ì¤‘...")

    # ì—…ë¡œë“œ (batchë¡œ ë‚˜ëˆ”)
    BATCH_SIZE = 100
    for i in range(0, len(all_docs), BATCH_SIZE):
        batch = all_docs[i:i + BATCH_SIZE]
        PineconeVectorStore.from_documents(batch, embedding_upstage, index_name=index_name)
        print(f"âœ… ì—…ë¡œë“œ: {i} ~ {i + len(batch) - 1}")

# 6. ì‹¤í–‰
print("ğŸš€ ëª¨ë“  Markdown ë¬¸ì„œë¥¼ ë‹¨ì¼ ì¸ë±ìŠ¤ì— ì—…ë¡œë“œ ì‹œì‘...")
upload_all_to_single_index("DB/", "cs-interview-index")
print("âœ… ì „ì²´ ì™„ë£Œ")
